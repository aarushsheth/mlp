{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Implement the network on the image and overfit the first batch (of 32 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s:idx + 1 for idx, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {idx: s for s, idx in stoi.items()}\n",
    "block_size = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3]) torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):  \n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "x, y = build_dataset(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(x)\n",
    "\n",
    "rand_idxs = torch.randperm(m)\n",
    "x = x[rand_idxs]\n",
    "y = y[rand_idxs]\n",
    "n1 = int(0.8 * m)\n",
    "n2 = int(0.9 * m)\n",
    "\n",
    "x_train = x[:n1]\n",
    "y_train = y[:n1]\n",
    "x_val = x[n1:n2]\n",
    "y_val = y[n1:n2]\n",
    "x_test = x[n2:]\n",
    "y_test = y[n2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.randn((27, 2))\n",
    "c[x_train].shape\n",
    "emb_train = c[x_train].view(-1, 6)\n",
    "emb_train.shape\n",
    "w1 = torch.randn((6, 100))\n",
    "b1 = torch.randn((1, 100))\n",
    "w1.shape\n",
    "w2 = torch.randn((100, 27))\n",
    "b2 = torch.randn((1, 27))\n",
    "c = torch.randn((27, 2))\n",
    "w1 = torch.randn((6, 100))\n",
    "b1 = torch.randn((1, 100))\n",
    "w2 = torch.randn((100, 27))\n",
    "b2 = torch.randn((1, 27))\n",
    "params = [c, w1, b1, w2, b2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "x_batch = x[: BATCH_SIZE, :]\n",
    "y_batch = y[: BATCH_SIZE]\n",
    "x_batch.shape\n",
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.10325682163238525\n"
     ]
    }
   ],
   "source": [
    "LEARN_RATE = 0.1\n",
    "N_ITERS = 10000\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "c = torch.randn((27, 2), generator=g)\n",
    "w1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "w2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "params = [c, w1, b1, w2, b2]\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "for k in range(N_ITERS):\n",
    "    emb = c[x_batch]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ w1 + b1)\n",
    "    logits = h @ w2 + b2\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in params:\n",
    "        p.data = p.data - LEARN_RATE * p.grad\n",
    "\n",
    "print(f\"loss: {loss.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Tune the hyperparameters of the training to beat my best validation loss of 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 16, 'learn_rate': 0.1}\n",
      "{'batch_size': 16, 'learn_rate': 0.05}\n",
      "{'batch_size': 16, 'learn_rate': 0.01}\n",
      "{'batch_size': 16, 'learn_rate': 0.001}\n",
      "{'batch_size': 32, 'learn_rate': 0.1}\n",
      "{'batch_size': 32, 'learn_rate': 0.05}\n",
      "{'batch_size': 32, 'learn_rate': 0.01}\n",
      "{'batch_size': 32, 'learn_rate': 0.001}\n",
      "{'batch_size': 64, 'learn_rate': 0.1}\n",
      "{'batch_size': 64, 'learn_rate': 0.05}\n",
      "{'batch_size': 64, 'learn_rate': 0.01}\n",
      "{'batch_size': 64, 'learn_rate': 0.001}\n",
      "{'batch_size': 128, 'learn_rate': 0.1}\n",
      "{'batch_size': 128, 'learn_rate': 0.05}\n",
      "{'batch_size': 128, 'learn_rate': 0.01}\n",
      "{'batch_size': 128, 'learn_rate': 0.001}\n",
      "{'batch_size': 256, 'learn_rate': 0.1}\n",
      "{'batch_size': 256, 'learn_rate': 0.05}\n",
      "{'batch_size': 256, 'learn_rate': 0.01}\n",
      "{'batch_size': 256, 'learn_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"batch_size\": [16, 32, 64, 128, 256],\n",
    "    \"learn_rate\": [0.1, 0.05, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = ParameterGrid(params)\n",
    "\n",
    "for p in param_grid:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:14<03:20, 14.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 16, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2741599082946777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:28<03:08, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 16, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.308612108230591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:43<02:53, 14.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 16, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.514777183532715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:58<02:43, 14.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 32, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.246750593185425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [01:14<02:31, 15.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 32, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.2910361289978027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [01:30<02:17, 15.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 32, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.5218966007232666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [01:49<02:11, 16.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 64, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2315168380737305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [02:07<02:00, 17.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 64, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.28310227394104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [02:26<01:45, 17.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 64, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.540093421936035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [02:50<01:38, 19.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 128, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2319602966308594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [03:15<01:24, 21.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 128, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.274763584136963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [03:40<01:07, 22.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 128, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.5039072036743164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [04:17<00:53, 26.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 256, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2291252613067627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [04:55<00:30, 30.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 256, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.266962766647339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [05:31<00:00, 22.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 256, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.514359474182129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "params = {\n",
    "    \"batch_size\": [16, 32, 64, 128, 256],\n",
    "    \"learn_rate\": [0.1, 0.05, 0.01],\n",
    "    \"num_iters\": [50000]\n",
    "}\n",
    "\n",
    "param_grid = ParameterGrid(params)\n",
    "\n",
    "for pset in tqdm(param_grid):\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    c = torch.randn((27, 10), generator=g, requires_grad=True)\n",
    "    w1 = torch.randn((30, 200), generator=g, requires_grad=True)\n",
    "    b1 = torch.randn(200, generator=g)\n",
    "    w2 = torch.randn((200, 27), generator=g)\n",
    "    b2 = torch.randn(27, generator=g)\n",
    "    params = [c, w1, b1, w2, b2]\n",
    "\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    for k in range(pset[\"num_iters\"]):\n",
    "        rand_idxs = torch.randint(0, x_train.shape[0], (pset[\"batch_size\"], ))\n",
    "        x_batch = x_train[rand_idxs]\n",
    "        y_batch = y_train[rand_idxs]\n",
    "\n",
    "        emb = c[x_batch]\n",
    "        h = torch.tanh(emb.view(-1, w1.shape[0]) @ w1 + b1)\n",
    "        logits = h @ w2 + b2\n",
    "        loss = F.cross_entropy(logits, y_batch)\n",
    "        \n",
    "        for p in params:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        lr = (pset[\"learn_rate\"] / 10) if k > pset[\"num_iters\"] / 2 else pset[\"learn_rate\"] \n",
    "        for p in params:\n",
    "            p.data = p.data - lr * p.grad\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = c[x_val]\n",
    "        h = torch.tanh(emb.view(-1, w1.shape[0]) @ w1 + b1)\n",
    "        logits = h @ w2 + b2\n",
    "        val_loss = F.cross_entropy(logits, y_val)\n",
    "        print(f\"pset: {pset}, val_loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "c = torch.randn((27, 10), generator=g, requires_grad=True)\n",
    "w1 = torch.randn((30, 200), generator=g, requires_grad=True)\n",
    "b1 = torch.randn(200, generator=g)\n",
    "w2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "params = [c, w1, b1, w2, b2]\n",
    "losses = []\n",
    "LEARN_RATE = 0.1\n",
    "BATCH_SIZE = 256\n",
    "N_ITERS = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.1909658908843994\n"
     ]
    }
   ],
   "source": [
    "for  p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "for k in range(N_ITERS):\n",
    "    rand_idxs = torch.randint(0, x_train.shape[0], (BATCH_SIZE, ))\n",
    "    x_batch = x_train[rand_idxs]\n",
    "    y_batch = y_train[rand_idxs]\n",
    "\n",
    "    emb = c[x_batch]\n",
    "    h = torch.tanh(emb.view(-1, w1.shape[0]) @ w1 + b1)\n",
    "    logits = h @ w2 + b2\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    if k > N_ITERS / 2:\n",
    "        lr = (LEARN_RATE / 10)\n",
    "    elif k > N_ITERS / 4:\n",
    "        lr = (LEARN_RATE / 100)\n",
    "    else:\n",
    "        lr = LEARN_RATE\n",
    "\n",
    "    for p in params:\n",
    "        p.data = p.data - lr * p.grad\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb = c[x_val]\n",
    "    h = torch.tanh(emb.view(-1, w1.shape[0]) @ w1 + b1)\n",
    "    logits = h @ w2 + b2\n",
    "    val_loss = F.cross_entropy(logits, y_val)\n",
    "    print(f\"val_loss: {val_loss.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(61616161)\n",
    "c = torch.randn((27, 2), generator=g)\n",
    "w1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "w2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.6983e-14, 1.1985e-04, 3.5999e-08,  ..., 1.9626e-08, 2.7613e-05,\n",
      "         1.2537e-03],\n",
      "        [6.0896e-11, 3.2000e-04, 1.4174e-09,  ..., 5.5973e-10, 1.0600e-03,\n",
      "         1.4048e-01],\n",
      "        [1.3202e-09, 2.3920e-09, 4.6479e-06,  ..., 3.2859e-11, 4.0674e-07,\n",
      "         1.1041e-03],\n",
      "        ...,\n",
      "        [1.5881e-17, 1.8376e-06, 5.6858e-12,  ..., 3.4350e-13, 2.2004e-09,\n",
      "         3.9131e-04],\n",
      "        [2.1407e-09, 1.1824e-07, 6.2860e-11,  ..., 1.5720e-11, 4.5586e-13,\n",
      "         5.6347e-07],\n",
      "        [7.1390e-09, 3.1715e-07, 4.8401e-06,  ..., 1.8034e-06, 1.2815e-07,\n",
      "         5.5459e-01]])\n",
      "initial loss: 15.979963302612305\n"
     ]
    }
   ],
   "source": [
    "emb = c[x_train]\n",
    "h = torch.tanh(emb.view(-1, w1.shape[0]) @ w1 + b1)\n",
    "logits = h @ w2 + b2\n",
    "print(torch.softmax(logits, dim=1))\n",
    "loss = F.cross_entropy(logits, y_train)\n",
    "\n",
    "print(f\"initial loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
      "        [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
      "        [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
      "        ...,\n",
      "        [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
      "        [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
      "        [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370]])\n",
      "initial loss: 3.29583740234375\n"
     ]
    }
   ],
   "source": [
    "c = torch.full((27, 10), fill_value=1.0)\n",
    "w1 = torch.full((30, 200), fill_value=0.02)\n",
    "b1 = torch.full((1,200), fill_value=0.0)\n",
    "w2 = torch.full((200, 27), fill_value=1.0)\n",
    "b2 = torch.full((1,27), fill_value=0.0)\n",
    "# Forward pass\n",
    "emb = c[x_train]\n",
    "h = torch.tanh(emb.view(-1, w1.shape[0]) @ w1 + b1)\n",
    "logits = h @ w2 + b2\n",
    "print(torch.softmax(logits, dim=1))\n",
    "loss = F.cross_entropy(logits, y_train)\n",
    "\n",
    "print(f\"initial loss: {loss.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(6161616161)\n",
    "c = torch.randn((27, 10), generator=g, requires_grad=True)\n",
    "w1 = torch.randn((30, 200), generator=g, requires_grad=True)\n",
    "b1 = torch.randn(200, generator=g, requires_grad=True)\n",
    "w2 = torch.randn((200, 27), generator=g, requires_grad=True)\n",
    "w0 = torch.randn((30, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn(27, generator=g, requires_grad=True)\n",
    "\n",
    "params = [c, w1, b1, w2, b2, w0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "x_batch = x[: BATCH_SIZE, :]\n",
    "y_batch = y[: BATCH_SIZE]\n",
    "x_batch.shape\n",
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.10337723791599274\n"
     ]
    }
   ],
   "source": [
    "LEARN_RATE = 0.1\n",
    "N_ITERS = 1000\n",
    "\n",
    "g = torch.Generator().manual_seed(32109832)\n",
    "c = torch.randn((27, 10), generator=g)\n",
    "w1 = torch.randn((30, 200), generator=g)\n",
    "b1 = torch.randn(200, generator=g)\n",
    "w2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "w0 = torch.randn((30, 27), generator=g)\n",
    "params = [c, w1, b1, w2, b2, w0]\n",
    "losses = []\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "for k in range(N_ITERS):\n",
    "    emb = c[x_batch]\n",
    "    h = torch.tanh(emb.view(-1, 3 * c.shape[1]) @ w1 + b1)\n",
    "    logits = h @ w2 + b2 + emb.view(-1, w1.shape[0]) @ w0\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "    losses.append(loss.item())\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in params:\n",
    "        p.data = p.data - LEARN_RATE * p.grad\n",
    "\n",
    "print(f\"loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:16<03:49, 16.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 16, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.258974313735962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:32<03:33, 16.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 16, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.2936909198760986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:49<03:17, 16.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 16, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.4179043769836426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [01:06<03:06, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 32, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2432901859283447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [01:24<02:53, 17.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 32, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.286076068878174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [01:43<02:38, 17.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 32, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.452589750289917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [02:04<02:30, 18.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 64, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.24276065826416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [02:25<02:16, 19.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 64, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.282175302505493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [02:46<01:59, 19.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 64, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.4560420513153076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [03:12<01:49, 21.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 128, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.236204147338867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [03:39<01:34, 23.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 128, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.27168607711792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [04:06<01:13, 24.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 128, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.4331679344177246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [05:20<01:18, 39.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 256, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2296738624572754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [06:01<00:39, 39.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 256, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.2757456302642822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [06:43<00:00, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 256, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.4330084323883057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "params = {\n",
    "    \"batch_size\": [16, 32, 64, 128, 256],\n",
    "    \"learn_rate\": [0.1, 0.05, 0.01],\n",
    "    \"num_iters\": [50000]\n",
    "}\n",
    "\n",
    "param_grid = ParameterGrid(params)\n",
    "\n",
    "for pset in tqdm(param_grid):\n",
    "    g = torch.Generator().manual_seed(127207)\n",
    "    c = torch.randn((27, 10), generator=g)\n",
    "    w1 = torch.randn((30, 200), generator=g)\n",
    "    b1 = torch.randn(200, generator=g)\n",
    "    w2 = torch.randn((200, 27), generator=g)\n",
    "    b2 = torch.randn(27, generator=g)\n",
    "    w0 = torch.randn((30, 27), generator=g)\n",
    "    params = [c, w1, b1, w2, b2, w0]\n",
    "\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    for k in range(pset[\"num_iters\"]):\n",
    "        rand_idxs = torch.randint(0, x_train.shape[0], (pset[\"batch_size\"], ))\n",
    "        x_batch = x_train[rand_idxs]\n",
    "        y_batch = y_train[rand_idxs]\n",
    "\n",
    "        emb = c[x_batch]\n",
    "        h = torch.tanh(emb.view(-1, 3 * c.shape[1]) @ w1 + b1)\n",
    "        logits = h @ w2 + b2 + emb.view(-1, w1.shape[0]) @ w0\n",
    "        loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "        for p in params:\n",
    "            p.grad = None\n",
    "\n",
    "        loss.backward()\n",
    "        lr = (pset[\"learn_rate\"] / 10) if k > pset[\"num_iters\"] / 2 else pset[\"learn_rate\"] \n",
    "        for p in params:\n",
    "            p.data = p.data - lr * p.grad\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = c[x_val]\n",
    "        h = torch.tanh(emb.view(-1, w1.shape[0]) @ w1 + b1)\n",
    "        logits = h @ w2 + b2 + emb.view(-1, w1.shape[0]) @ w0\n",
    "        val_loss = F.cross_entropy(logits, y_val)\n",
    "        print(f\"pset: {pset}, val_loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:23<05:30, 23.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 16, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2648887634277344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:47<05:06, 23.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 16, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.2890281677246094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [01:11<04:44, 23.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 16, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.494035482406616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [01:35<04:25, 24.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 32, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2565503120422363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [02:00<04:04, 24.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 32, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.2891697883605957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [02:25<03:42, 24.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 32, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.4573726654052734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [02:56<03:33, 26.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 64, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.239196538925171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [03:26<03:13, 27.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 64, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.2776100635528564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [03:55<02:48, 28.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 64, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.453270435333252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [04:29<02:29, 29.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 128, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2315444946289062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [05:03<02:04, 31.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 128, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.2656126022338867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [05:39<01:37, 32.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 128, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.448518991470337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [06:28<01:15, 37.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 256, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.229033946990967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [07:15<00:40, 40.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 256, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.2734968662261963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [08:09<00:00, 32.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 256, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.427614688873291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "params = {\n",
    "    \"batch_size\": [16, 32, 64, 128, 256],\n",
    "    \"learn_rate\": [0.1, 0.05, 0.01],\n",
    "    \"num_iters\": [50000]\n",
    "}\n",
    "\n",
    "param_grid = ParameterGrid(params)\n",
    "reg_strength = 0.01\n",
    "\n",
    "for pset in tqdm(param_grid):\n",
    "    g = torch.Generator().manual_seed(613726192)\n",
    "    c = torch.randn((27, 10), generator=g)\n",
    "    w1 = torch.randn((30, 200), generator=g)\n",
    "    b1 = torch.randn(200, generator=g)\n",
    "    w2 = torch.randn((200, 27), generator=g)\n",
    "    b2 = torch.randn(27, generator=g)\n",
    "    w0 = torch.randn((30, 27), generator=g)\n",
    "    params = [c, w1, b1, w2, b2, w0]\n",
    "\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "    for k in range(pset[\"num_iters\"]):\n",
    "        rand_idxs = torch.randint(0, x_train.shape[0], (pset[\"batch_size\"], ))\n",
    "        x_batch = x_train[rand_idxs]\n",
    "        y_batch = y_train[rand_idxs]\n",
    "\n",
    "        emb = c[x_batch]\n",
    "        h = torch.tanh(emb.view(-1, 3 * c.shape[1]) @ w1 + b1)\n",
    "        logits = h @ w2 + b2 + emb.view(-1, w1.shape[0]) @ w0\n",
    "        loss = F.cross_entropy(logits, y_batch) + reg_strength * (w0.pow(2).mean() + w1.pow(2).mean() + w2.pow(2).mean())\n",
    "\n",
    "        for p in params:\n",
    "            p.grad = None\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        lr = (pset[\"learn_rate\"] / 10) if k > pset[\"num_iters\"] / 2 else pset[\"learn_rate\"] \n",
    "        for p in params:\n",
    "            p.data = p.data - lr * p.grad\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = c[x_val]\n",
    "        h = torch.tanh(emb.view(-1, w1.shape[0]) @ w1 + b1)\n",
    "        logits = h @ w2 + b2 + emb.view(-1, w1.shape[0]) @ w0\n",
    "        val_loss = F.cross_entropy(logits, y_val)\n",
    "        print(f\"pset: {pset}, val_loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:27<06:27, 27.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 16, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2670199871063232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:56<06:05, 28.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 16, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.2923951148986816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [01:25<05:43, 28.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 16, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.422217607498169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [01:53<05:14, 28.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 32, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2561957836151123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [02:22<04:44, 28.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 32, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.289093494415283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [02:51<04:18, 28.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 32, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.408313751220703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [03:23<04:00, 30.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 64, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2334611415863037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [03:56<03:35, 30.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 64, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.2751669883728027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [04:28<03:06, 31.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 64, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.4027657508850098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [05:05<02:45, 33.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 128, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.23679518699646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [05:43<02:17, 34.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 128, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.266667604446411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [06:22<01:47, 35.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 128, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.4048216342926025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [07:13<01:21, 40.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 256, 'learn_rate': 0.1, 'num_iters': 50000}, val_loss: 2.2498393058776855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [08:04<00:43, 43.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 256, 'learn_rate': 0.05, 'num_iters': 50000}, val_loss: 2.2753329277038574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [08:55<00:00, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pset: {'batch_size': 256, 'learn_rate': 0.01, 'num_iters': 50000}, val_loss: 2.406517505645752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "params = {\n",
    "    \"batch_size\": [16, 32, 64, 128, 256],\n",
    "    \"learn_rate\": [0.1, 0.05, 0.01],\n",
    "    \"num_iters\": [50000]\n",
    "}\n",
    "\n",
    "param_grid = ParameterGrid(params)\n",
    "reg_strength = 0.01\n",
    "beta = 0.9 \n",
    "\n",
    "for pset in tqdm(param_grid):\n",
    "    g = torch.Generator().manual_seed(61616161616161611)\n",
    "    c = torch.randn((27, 10), generator=g)\n",
    "    w1 = torch.randn((30, 200), generator=g)\n",
    "    b1 = torch.randn(200, generator=g)\n",
    "    w2 = torch.randn((200, 27), generator=g)\n",
    "    b2 = torch.randn(27, generator=g)\n",
    "    w0 = torch.randn((30, 27), generator=g)\n",
    "    params = [c, w1, b1, w2, b2, w0]\n",
    "    param_grads = [0.0 for _ in range(len(params))]\n",
    "\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    for k in range(pset[\"num_iters\"]):\n",
    "        rand_idxs = torch.randint(0, x_train.shape[0], (pset[\"batch_size\"], ))\n",
    "        x_batch = x_train[rand_idxs]\n",
    "        y_batch = y_train[rand_idxs]\n",
    "\n",
    "        emb = c[x_batch]\n",
    "        h = torch.tanh(emb.view(-1, 3 * c.shape[1]) @ w1 + b1)\n",
    "        logits = h @ w2 + b2 + emb.view(-1, w1.shape[0]) @ w0\n",
    "        loss = F.cross_entropy(logits, y_batch) + reg_strength * (w0.pow(2).mean() + w1.pow(2).mean() + w2.pow(2).mean())\n",
    "\n",
    "        for p in params:\n",
    "            p.grad = None\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        lr = (pset[\"learn_rate\"] / 10) if k > pset[\"num_iters\"] / 2 else pset[\"learn_rate\"] \n",
    "        for idx, p in enumerate(params):\n",
    "            param_grads[idx] = beta * param_grads[idx] + (1 - beta) * p.grad\n",
    "            p.data = p.data - lr * param_grads[idx]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = c[x_val]\n",
    "        h = torch.tanh(emb.view(-1, w1.shape[0]) @ w1 + b1)\n",
    "        logits = h @ w2 + b2 + emb.view(-1, w1.shape[0]) @ w0\n",
    "        val_loss = F.cross_entropy(logits, y_val)\n",
    "        print(f\"pset: {pset}, val_loss: {val_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(6161616161116)\n",
    "c = torch.randn((27, 10), generator=g)\n",
    "w1 = torch.randn((30, 200), generator=g)\n",
    "b1 = torch.randn(200, generator=g)\n",
    "w2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "w0 = torch.randn((30, 27), generator=g)\n",
    "params = [c, w1, b1, w2, b2, w0]\n",
    "param_grads = [0.0 for _ in range(len(params))]\n",
    "LEARN_RATE = 0.1\n",
    "BATCH_SIZE = 256\n",
    "N_ITERS = 200000\n",
    "BETA = 0.9 \n",
    "REG_STRENGTH = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.193235397338867\n"
     ]
    }
   ],
   "source": [
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "for k in range(N_ITERS):\n",
    "    rand_idxs = torch.randint(0, x_train.shape[0], (BATCH_SIZE, ))\n",
    "    x_batch = x_train[rand_idxs]\n",
    "    y_batch = y_train[rand_idxs]\n",
    "\n",
    "    emb = c[x_batch]\n",
    "    h = torch.tanh(emb.view(-1, 3 * c.shape[1]) @ w1 + b1)\n",
    "    logits = h @ w2 + b2 + emb.view(-1, w1.shape[0]) @ w0\n",
    "    loss = F.cross_entropy(logits, y_batch) + REG_STRENGTH * (w0.pow(2).mean() + w1.pow(2).mean() + w2.pow(2).mean())\n",
    "\n",
    "    \n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    if k > N_ITERS / 2:\n",
    "        lr = (LEARN_RATE / 10)\n",
    "    elif k > N_ITERS / 4:\n",
    "        lr = (LEARN_RATE / 100)\n",
    "    else:\n",
    "        lr = LEARN_RATE\n",
    "\n",
    "    for idx, p in enumerate(params):\n",
    "        param_grads[idx] = BETA * param_grads[idx] + (1 - beta) * p.grad\n",
    "        p.data = p.data - lr * param_grads[idx]\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb = c[x_val]\n",
    "    h = torch.tanh(emb.view(-1, w1.shape[0]) @ w1 + b1)\n",
    "    logits = h @ w2 + b2 + emb.view(-1, w1.shape[0]) @ w0\n",
    "    val_loss = F.cross_entropy(logits, y_val)\n",
    "    print(f\"val_loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kassela.\n",
      "marvionna.\n",
      "aver.\n",
      "abriegh.\n",
      "tay.\n",
      "larika.\n",
      "aiburlet.\n",
      "keew.\n",
      "kenex.\n",
      "ashellen.\n",
      "mila.\n",
      "metalo.\n",
      "cryl.\n",
      "ketina.\n",
      "evievi.\n",
      "xokiriah.\n",
      "mikannah.\n",
      "brity.\n",
      "fer.\n",
      "elis.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(694390)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size \n",
    "    while True:\n",
    "      emb = c[torch.tensor([context])] \n",
    "      h = torch.tanh(emb.view(1, -1) @ w1 + b1)\n",
    "      logits = h @ w2 + b2 + emb.view(-1, w1.shape[0]) @ w0\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
